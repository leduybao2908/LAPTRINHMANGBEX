# Naive Bayes Spam Detection - Chi tiáº¿t ká»¹ thuáº­t

## ğŸ¯ Tá»•ng quan

Há»‡ thá»‘ng Ä‘Ã£ Ä‘Æ°á»£c nÃ¢ng cáº¥p tá»« **keyword-based** Ä‘Æ¡n giáº£n sang **Naive Bayes classifier** - má»™t thuáº­t toÃ¡n Machine Learning máº¡nh máº½ cho text classification.

---

## ğŸ“Š So sÃ¡nh: Keyword-based vs Naive Bayes

| TiÃªu chÃ­ | Keyword-based | Naive Bayes |
|----------|---------------|-------------|
| **Thuáº­t toÃ¡n** | Äáº¿m keywords | Probabilistic ML |
| **Äá»™ chÃ­nh xÃ¡c** | ~60-70% | ~85-95% |
| **Training** | KhÃ´ng cáº§n | Cáº§n training data |
| **False positive** | Cao | Tháº¥p |
| **Context-aware** | âŒ KhÃ´ng | âœ… CÃ³ (pháº§n nÃ o) |
| **Tá»‘c Ä‘á»™** | Ráº¥t nhanh | Nhanh |
| **Vocabulary** | 20 keywords | ToÃ n bá»™ vocabulary |

---

## ğŸ§® Thuáº­t toÃ¡n Naive Bayes

### 1. CÃ´ng thá»©c cÆ¡ báº£n

```
P(Spam|Email) = P(Email|Spam) * P(Spam) / P(Email)
```

**Trong Ä‘Ã³:**
- `P(Spam|Email)`: XÃ¡c suáº¥t email lÃ  spam khi biáº¿t ná»™i dung
- `P(Email|Spam)`: XÃ¡c suáº¥t gáº·p ná»™i dung nÃ y trong spam
- `P(Spam)`: XÃ¡c suáº¥t prior cá»§a spam
- `P(Email)`: XÃ¡c suáº¥t gáº·p ná»™i dung nÃ y

### 2. Naive Assumption

**"Naive" = Giáº£ Ä‘á»‹nh ngÃ¢y thÆ¡**: CÃ¡c tá»« trong email Ä‘á»™c láº­p vá»›i nhau.

```
P(Email|Spam) = P(word1|Spam) * P(word2|Spam) * ... * P(wordN|Spam)
```

Máº·c dÃ¹ giáº£ Ä‘á»‹nh nÃ y khÃ´ng Ä‘Ãºng 100% (cÃ¡c tá»« cÃ³ liÃªn quan), nhÆ°ng trong thá»±c táº¿ Naive Bayes váº«n hoáº¡t Ä‘á»™ng ráº¥t tá»‘t!

### 3. Multinomial Naive Bayes

Sá»­ dá»¥ng cho text classification (Ä‘áº¿m sá»‘ láº§n xuáº¥t hiá»‡n cá»§a tá»«):

```
P(word|Spam) = (count(word in spam) + 1) / (total spam words + vocabulary size)
```

**+1** lÃ  **Laplace Smoothing** Ä‘á»ƒ trÃ¡nh xÃ¡c suáº¥t = 0 vá»›i tá»« chÆ°a gáº·p.

---

## ğŸ’¾ Training Data

Há»‡ thá»‘ng Ä‘Æ°á»£c train vá»›i **40 emails**:

### Spam emails (20):
```
- "Congratulations! You won $1000000 free prize money..."
- "URGENT! Your account will be closed. Click here..."
- "Win free iPhone! Click here to claim your prize..."
- ... (17 emails khÃ¡c)
```

### Ham emails (20):
```
- "Hi, let's meet tomorrow at 10am to discuss the project..."
- "Please review the attached document and send feedback..."
- "Thank you for your help with the presentation..."
- ... (17 emails khÃ¡c)
```

---

## ğŸ”¬ Chi tiáº¿t implementation

### 1. Tokenization
```java
// Chuyá»ƒn text thÃ nh list of words
"Win Free Money!" 
â†’ ["win", "free", "money"]

// Lowercase, remove punctuation, filter short words
```

### 2. Training Phase
```java
// Äáº¿m tá»« trong spam
spamWordCount.put("free", 15);  // "free" xuáº¥t hiá»‡n 15 láº§n trong spam
spamWordCount.put("money", 12);
totalSpamWords = 500;

// Äáº¿m tá»« trong ham
hamWordCount.put("meeting", 8);
hamWordCount.put("project", 10);
totalHamWords = 480;
```

### 3. Classification Phase

**VÃ­ dá»¥: Classify "Win free money now!"**

```java
// Step 1: Prior probabilities
P(Spam) = 20 / 40 = 0.5
P(Ham) = 20 / 40 = 0.5

// Step 2: Word probabilities (with Laplace smoothing)
P("win"|Spam) = (8 + 1) / (500 + 1000) = 0.006
P("free"|Spam) = (15 + 1) / (500 + 1000) = 0.0107
P("money"|Spam) = (12 + 1) / (500 + 1000) = 0.0087

P("win"|Ham) = (0 + 1) / (480 + 1000) = 0.00067
P("free"|Ham) = (2 + 1) / (480 + 1000) = 0.002
P("money"|Ham) = (1 + 1) / (480 + 1000) = 0.00135

// Step 3: Calculate log probabilities (trÃ¡nh underflow)
log P(Spam|Email) = log(0.5) + log(0.006) + log(0.0107) + log(0.0087)
                  = -0.693 + (-5.116) + (-4.539) + (-4.744)
                  = -15.092

log P(Ham|Email) = log(0.5) + log(0.00067) + log(0.002) + log(0.00135)
                 = -0.693 + (-7.308) + (-6.215) + (-6.608)
                 = -20.824

// Step 4: Normalize to get probability
P(Spam|Email) = exp(-15.092) / (exp(-15.092) + exp(-20.824))
              â‰ˆ 0.997 (99.7% spam)

â†’ SPAM! âš ï¸
```

---

## ğŸ“ˆ Confidence Levels

Há»‡ thá»‘ng phÃ¢n loáº¡i theo má»©c Ä‘á»™ tin cáº­y:

| Spam Score | Classification | MÃ´ táº£ |
|------------|----------------|-------|
| 0.0 - 0.3 | **Definitely Ham** | Cháº¯c cháº¯n clean |
| 0.3 - 0.5 | **Probably Ham** | CÃ³ thá»ƒ clean |
| 0.5 - 0.7 | **Probably Spam** | CÃ³ thá»ƒ spam |
| 0.7 - 1.0 | **Definitely Spam** | Cháº¯c cháº¯n spam |

---

## ğŸ§ª Test Cases

### Test 1: Spam rÃµ rÃ ng
```
Subject: "Win Free Money Prize!"
Body: "Click here to claim your reward now! Limited offer!"

â†’ Spam Score: 0.95 (95%)
â†’ Classification: Definitely Spam âš ï¸
```

### Test 2: Ham rÃµ rÃ ng
```
Subject: "Meeting tomorrow"
Body: "Let's discuss the project plan at 10am"

â†’ Spam Score: 0.08 (8%)
â†’ Classification: Definitely Ham âœ…
```

### Test 3: Borderline case
```
Subject: "Free coffee for team"
Body: "I'll bring free coffee for everyone tomorrow"

â†’ Spam Score: 0.45 (45%)
â†’ Classification: Probably Ham âœ…

(DÃ¹ cÃ³ "free" nhÆ°ng context lÃ  há»£p lá»‡)
```

### Test 4: Tricky spam
```
Subject: "Important notification"
Body: "You won a prize. Contact us to claim your reward."

â†’ Spam Score: 0.78 (78%)
â†’ Classification: Definitely Spam âš ï¸

(Model nháº­n ra pattern "won", "prize", "claim", "reward")
```

---

## âš™ï¸ Cáº¥u hÃ¬nh

### Thay Ä‘á»•i threshold
```java
NaiveBayesSpamDetector detector = NaiveBayesSpamDetector.getInstance();
detector.setSpamThreshold(0.7);  // Máº·c Ä‘á»‹nh: 0.5

// 0.7 = Less sensitive (fewer false positives)
// 0.3 = More sensitive (catch more spam, but more false positives)
```

### ThÃªm training data
```java
// ThÃªm spam email
detector.train("New spam email content...", true);

// ThÃªm ham email  
detector.train("New legitimate email content...", false);
```

---

## ğŸ“Š Model Statistics

Xem thá»‘ng kÃª model:
```java
System.out.println(detector.getModelStats());
```

Output:
```
Naive Bayes Model:
- Spam emails trained: 20
- Ham emails trained: 20
- Spam vocabulary: 156 words
- Ham vocabulary: 142 words
- Total vocabulary: 278 words
- Threshold: 0.50
```

---

## ğŸ¯ Æ¯u Ä‘iá»ƒm Naive Bayes

âœ… **ChÃ­nh xÃ¡c cao hÆ¡n keyword-based**
- Hiá»ƒu context tá»‘t hÆ¡n
- Ãt false positive

âœ… **Nhanh**
- O(V) complexity (V = vocabulary size)
- KhÃ´ng cáº§n neural network phá»©c táº¡p

âœ… **Ãt data cáº§n thiáº¿t**
- Hoáº¡t Ä‘á»™ng tá»‘t vá»›i 20-40 emails
- KhÃ´ng cáº§n thousands of examples nhÆ° deep learning

âœ… **Dá»… train thÃªm**
- Incremental learning
- CÃ³ thá»ƒ update model liÃªn tá»¥c

âœ… **Explainable**
- CÃ³ thá»ƒ show tá»« nÃ o contribute vÃ o spam score
- KhÃ´ng pháº£i "black box"

---

## âš ï¸ Háº¡n cháº¿

âŒ **Naive assumption**
- Giáº£ Ä‘á»‹nh tá»« Ä‘á»™c láº­p (khÃ´ng hoÃ n toÃ n Ä‘Ãºng)
- "Free money" khÃ¡c "money free"

âŒ **Cáº§n training data**
- Pháº£i cÃ³ spam + ham examples
- Quality of training data matters

âŒ **Language-specific**
- Model train tiáº¿ng Anh khÃ´ng work vá»›i tiáº¿ng Viá»‡t
- Cáº§n train riÃªng cho má»—i ngÃ´n ngá»¯

âŒ **KhÃ´ng hiá»ƒu obfuscation**
- "Fr33 m0n3y" cÃ³ thá»ƒ bypass
- Cáº§n pre-processing phá»©c táº¡p hÆ¡n

---

## ğŸš€ Cáº£i tiáº¿n cÃ³ thá»ƒ lÃ m

### Level 1: Easy
```java
// ThÃªm n-grams (2-3 words together)
"free money" â†’ treat as single token
"click here" â†’ treat as single token
```

### Level 2: Medium
```java
// Feature engineering
- Email length
- Number of exclamation marks!!!
- ALL CAPS words
- Presence of URLs
- Sender reputation
```

### Level 3: Advanced
```java
// Combine with other models
- Ensemble: Naive Bayes + SVM + Random Forest
- Deep Learning: LSTM, BERT
- Active Learning: Learn from user feedback
```

---

## ğŸ“š So sÃ¡nh vá»›i cÃ¡c thuáº­t toÃ¡n khÃ¡c

| Algorithm | Accuracy | Speed | Training Data | Interpretability |
|-----------|----------|-------|---------------|------------------|
| **Keyword-based** | 60-70% | âš¡âš¡âš¡ | None | âœ…âœ…âœ… High |
| **Naive Bayes** | 85-95% | âš¡âš¡ | Low | âœ…âœ… Medium |
| **SVM** | 90-96% | âš¡ | Medium | âœ… Low |
| **Random Forest** | 88-94% | âš¡âš¡ | Medium | âœ… Low |
| **Deep Learning** | 95-99% | âš¡ | High | âŒ Very Low |

**â†’ Naive Bayes lÃ  sweet spot: Good accuracy + Fast + Easy to understand**

---

## ğŸ“ TÃ i liá»‡u tham kháº£o

- [Naive Bayes Classifier - Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)
- [Paul Graham's "A Plan for Spam"](http://www.paulgraham.com/spam.html) - Classic paper
- [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)

---

**Version**: 0.1.0
**Date**: 23/12/2025
**Author**: Mail Chat System Team
